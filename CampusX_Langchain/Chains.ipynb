{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc5bdcb",
   "metadata": {},
   "source": [
    "# Simple Chain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498becdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are 5 interesting facts about apples:\\n\\n1. **Apples are technically berries!**  This is because they develop from a single ovary of a flower and contain seeds.  \\n2. **Ancient Romans used apples for currency.**  They were so valuable that they even had a saying, \"An apple a day keeps the doctor away,\" which dates back to Roman times.\\n3. **The world\\'s largest apple tree is in the U.S.** Located in  Bartlett, Tennessee, this apple tree is estimated to be over 100 years old and covers an acre of land. \\n4. **There are over 7,500 varieties of apples!**  While only a few are widely popular, there\\'s a huge diversity of apples out there, each with unique flavors and textures. \\n5. **Apples float!**  Due to their air pockets, apples are less dense than water and will float.\\n\\n\\n\\nLet me know if you\\'d like to know more about apples! ðŸŽ \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='Generate 5 interesting facts about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"topic\": \"apple\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25883c",
   "metadata": {},
   "source": [
    "### Visualize the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e3469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grandalf\n",
      "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyparsing in d:\\python_env\\langchain\\venv\\lib\\site-packages (from grandalf) (3.2.3)\n",
      "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "Installing collected packages: grandalf\n",
      "Successfully installed grandalf-0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbbb7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +----------+         \n",
      "      | ChatGroq |         \n",
      "      +----------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144407b",
   "metadata": {},
   "source": [
    "# Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955157b5",
   "metadata": {},
   "source": [
    "Topic â†’ Detailed Report â†’ LLM â†’ Prompt 2 â†’ 5 point summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate a 5 pointer summary from the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109d8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d269fe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a 5-point summary of the Apple Inc. report:\\n\\n1. **Evolution of a Giant:** Apple's journey began with personal computers, evolved through the iPod and iPhone revolutions, and now encompasses a diverse ecosystem of hardware and software.\\n2. **Powerful Business Model:** Apple's success stems from selling premium hardware, a thriving software and services ecosystem (App Store, iCloud, etc.), and strong brand loyalty that drives repeat purchases.\\n3. **Exceptional Financial Performance:** Apple consistently reports high revenue and profits, maintains significant cash reserves, and boasts a highly valuable stock.\\n4. **Navigating a Competitive Landscape:** While facing rivals like Samsung, Google, and Microsoft, Apple differentiates itself through its brand, design, ecosystem integration, and customer relationships.\\n5. **Future Focus and Challenges:** Apple is poised for growth through continued innovation in areas like AI and AR, expanding its services, and targeting emerging markets. However, it must overcome supply chain issues, intense competition, and economic uncertainties.\\n\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt1 | model | parser | prompt2 | model | parser\n",
    "\n",
    "chain.invoke({\"topic\": \"apple\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc317dac",
   "metadata": {},
   "source": [
    "# Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef41063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()\n",
    "\n",
    "model1 = ChatGroq(model=\"gemma2-9b-it\")\n",
    "model2= ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate short and simple notes from the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate 5 short question answers from the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8229f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3= PromptTemplate(\n",
    "    template=\"Merge the provided notes and quiz into a single document \\n notes: {notes} and {quiz}\",\n",
    "    input_variables=[\"notes\", \"quiz\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560e69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831395a",
   "metadata": {},
   "source": [
    "### Make a parallel chain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e83219",
   "metadata": {},
   "source": [
    "Make 2 chains & pass them in RunnableParallel & name them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8cb0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "notes = prompt1 | model | parser\n",
    "\n",
    "quiz = prompt2 | model | parser\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"notes\": notes,\n",
    "    \"quiz\": quiz\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df1f60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chain = prompt3 | model2 | parser\n",
    "\n",
    "chain = parallel_chain | merge_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f38b9a",
   "metadata": {},
   "source": [
    "Invoke the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c32a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "Effective in high dimensional spaces.\n",
    "\n",
    "Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "\n",
    "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "\n",
    "SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    "\n",
    "The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.\n",
    "\"\"\"\n",
    "\n",
    "result =chain.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d0a7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Support Vector Machines (SVMs) - Notes and Quiz\n",
      "\n",
      "### What are SVMs?\n",
      "\n",
      "* Supervised learning methods for:\n",
      "    * Classification\n",
      "    * Regression\n",
      "    * Outlier detection\n",
      "\n",
      "### Advantages:\n",
      "\n",
      "* Effective in high dimensional spaces.\n",
      "* Work well even when dimensions > samples.\n",
      "* Memory efficient (uses a subset of training data - support vectors).\n",
      "* Versatile: different kernel functions can be used.\n",
      "\n",
      "### Disadvantages:\n",
      "\n",
      "* **Overfitting risk:** When features >> samples, careful kernel selection and regularization are crucial.\n",
      "* **No direct probability estimates:** Calculated using expensive cross-validation.\n",
      "\n",
      "### Input Data:\n",
      "\n",
      "* Can handle dense (numpy arrays) or sparse (scipy sparse) vectors.\n",
      "* **Important:** Must train on the same data type you want to predict with.\n",
      "* **Best performance:** C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.\n",
      "\n",
      "### Quiz\n",
      "\n",
      "**1. What are Support Vector Machines (SVMs) used for?**\n",
      "**Answer:** SVMs are used for classification, regression, and outlier detection.\n",
      "\n",
      "**2. What is an advantage of SVMs in terms of memory usage?**\n",
      "**Answer:** SVMs use only a subset of training points (support vectors) in the decision function, making them memory efficient.\n",
      "\n",
      "**3. What is a potential disadvantage of SVMs when the number of features is high?**\n",
      "**Answer:** Overfitting can occur if the kernel functions and regularization term are not carefully chosen.\n",
      "\n",
      "**4. How are probability estimates obtained from SVMs?**\n",
      "**Answer:** Probability estimates are calculated using expensive cross-validation, specifically five-fold cross-validation.\n",
      "\n",
      "**5. What type of input data can SVMs handle?**\n",
      "**Answer:** SVMs support both dense (numpy arrays) and sparse (scipy sparse matrices) sample vectors.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb0f53d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          +---------------------------+            \n",
      "          | Parallel<notes,quiz>Input |            \n",
      "          +---------------------------+            \n",
      "                ***             ***                \n",
      "              **                   **              \n",
      "            **                       **            \n",
      "+----------------+              +----------------+ \n",
      "| PromptTemplate |              | PromptTemplate | \n",
      "+----------------+              +----------------+ \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "    +----------+                  +----------+     \n",
      "    | ChatGroq |                  | ChatGroq |     \n",
      "    +----------+                  +----------+     \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "+-----------------+            +-----------------+ \n",
      "| StrOutputParser |            | StrOutputParser | \n",
      "+-----------------+            +-----------------+ \n",
      "                ***             ***                \n",
      "                   **         **                   \n",
      "                     **     **                     \n",
      "          +----------------------------+           \n",
      "          | Parallel<notes,quiz>Output |           \n",
      "          +----------------------------+           \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                +----------------+                 \n",
      "                | PromptTemplate |                 \n",
      "                +----------------+                 \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                   +----------+                    \n",
      "                   | ChatGroq |                    \n",
      "                   +----------+                    \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                +-----------------+                \n",
      "                | StrOutputParser |                \n",
      "                +-----------------+                \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "            +-----------------------+              \n",
      "            | StrOutputParserOutput |              \n",
      "            +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea7f66",
   "metadata": {},
   "source": [
    "# Conditional Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be6c4bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19e666db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25a0a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template= \"Classify the feedback into positive or negative. \\n feedback : {feedback}\",\n",
    "    input_variables= [\"feedback\"]\n",
    ")\n",
    "\n",
    "classifier_chain = prompt1 | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "daee1dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The feedback \"This is a terrible smartphone\" is **negative**. \\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_chain.invoke({\"feedback\": \"this is terrible smartphone\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3b5a8",
   "metadata": {},
   "source": [
    "### Convert this to structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f68a527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    sentiment: Literal['positive', 'negative'] = Field (description=\"Give the sentiment of the feedback\")\n",
    "\n",
    "parser2 = PydanticOutputParser(pydantic_object=Feedback)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c8c1e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment='negative'\n"
     ]
    }
   ],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template= \"Classify the feedback into positive or negative. \\n feedback : {feedback} \\n {format_instruction}\",\n",
    "    input_variables= [\"feedback\"],\n",
    "    partial_variables= {\"format_instruction\": parser2.get_format_instructions()}\n",
    ")\n",
    "\n",
    "classifier_chain = prompt1 | model | parser2\n",
    "\n",
    "print(classifier_chain.invoke({\"feedback\": \"this is terrible smartphone\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5241ae",
   "metadata": {},
   "source": [
    "### Branching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6517e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_fb = PromptTemplate(\n",
    "    template='Write an appropriate response to this positive feedback \\n {feedback}',\n",
    "    input_variables=['feedback']\n",
    ")\n",
    "\n",
    "neg_fb = PromptTemplate(\n",
    "    template='Write an appropriate response to this negative feedback \\n {feedback}',\n",
    "    input_variables=['feedback']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a895627",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x.sentiment == 'positive', pos_fb | model2 | parser),\n",
    "    (lambda x: x.sentiment == 'negative', neg_fb | model2 | parser),\n",
    "    RunnableLambda (lambda x: \"could not find sentiment\")  # default branch if none match\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c920585",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = classifier_chain | branch_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6fd811ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I'm sorry to hear that our product/service didn't meet your expectations. Can you please provide more details about your experience so we can better understand what went wrong? This will help us to improve our product/service and prevent similar issues in the future.\"\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'feedback': 'This is a terrible phone'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "456077d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+      \n",
      "    | PromptInput |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "   +----------------+    \n",
      "   | PromptTemplate |    \n",
      "   +----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "      +----------+       \n",
      "      | ChatGroq |       \n",
      "      +----------+       \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+----------------------+ \n",
      "| PydanticOutputParser | \n",
      "+----------------------+ \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "       +--------+        \n",
      "       | Branch |        \n",
      "       +--------+        \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +--------------+     \n",
      "    | BranchOutput |     \n",
      "    +--------------+     \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ece3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
